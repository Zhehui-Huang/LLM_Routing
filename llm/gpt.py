import sys
import os
import argparse
import time
import pytz
from datetime import datetime

from openai import OpenAI
from utils import (ask_llm, LLM_SYSTEM_PROMPT, list_files, extract_value, read_txt_file, write_py_file, run_py_file,
                   limit_text, check_correct_in_file, extract_python_code, check_log_file_empty)

LA_TIMEZONE = pytz.timezone('America/Los_Angeles')

SINGLE_TASK_LIST = ['TSP', 'BTSP', 'GTSP', 'KTSP', 'MV-TSP']
MULTI_TASK_LIST = ['mTSP', 'mTSP_MinMax', 'mTSPMD', 'CVRP']
# CITY_NUM_LIST = [10, 15, 20, 25, 50]
CITY_NUM_LIST = [10]
MAXIMUM_EXEC_TIME = 600
MAXIMUM_TEXT_LENGTH = 1000

INSTANCE_TRY_TIMES = 5

BASE_PATH = '/Users/tencentintern/Documents/LLM_Routing/llm'

OPENAI_API_KEY = "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"


def write_start_info(track_file_path, file_base_name, base_exec_details_path, task_name, city_num,
                     base_messages_path, instance_tid, outer_tid):
    # 1. Track info
    with open(track_file_path, 'a') as file:
        file.write(f"File: {file_base_name}\n+++\n")
    # 2 Execution results
    exec_detail_path = f'{base_exec_details_path}/{task_name}/{city_num}/{instance_tid}/{outer_tid}/exec_details_{file_base_name}.txt'
    os.makedirs(os.path.dirname(exec_detail_path), exist_ok=True)

    start_time = time.time()
    cur_time = datetime.now(LA_TIMEZONE)
    print(f"[{cur_time.strftime('%Y-%m-%d %H:%M:%S')}]\tStarting ...")
    with open(exec_detail_path, 'w') as file:
        file.write(f"Start time: [{cur_time.strftime('%Y-%m-%d %H:%M:%S')}]\n\n")
        file.write(f"Task file path: {exec_detail_path}\n===\n")

    # 3 Message info
    messages_path = f'{base_messages_path}/{task_name}/{city_num}/{instance_tid}/{outer_tid}/messages_{file_base_name}.txt'
    os.makedirs(os.path.dirname(messages_path), exist_ok=True)
    with open(messages_path, 'w') as file:
        file.write(f"Task file path: {messages_path}\n===\n")

    return exec_detail_path, start_time, messages_path


def write_end_info(start_time, exec_detail_path, track_file_path, tmp_reflect_num, tmp_log_file_path):
    # 1. Exec details
    end_time = time.time()
    response_time = end_time - start_time
    cur_time = datetime.now(LA_TIMEZONE)
    print(f"[{cur_time.strftime('%Y-%m-%d %H:%M:%S')}]\tFinished.\tOverall time: {response_time:.2f} seconds.")
    with open(exec_detail_path, 'a') as file:
        file.write(f"[{cur_time.strftime('%Y-%m-%d %H:%M:%S')}]\tFinished.\n\n")
        file.write(f"Overall time: {response_time:.2f} seconds.\n")

    # 2. Track info
    with open(track_file_path, 'a') as file:
        file.write(f"Reflect num: {tmp_reflect_num}\n")
        file.write(f"Log of solution file path: {tmp_log_file_path}\n+++\n\n\n")


def task2exec_res(args, client, file_base_name, task_name, city_num, messages, base_solution_path, base_log_path,
                  exec_detail_path, messages_path, instance_tid, outer_tid):
    tmp_reflect_num = 0
    tmp_log_file_path = None
    for i in range(args.reflect_num):
        print(f"Reflect count: {i}")
        with open(exec_detail_path, 'a') as file:
            file.write(f"\nReflect count: {i}\n===\n")

        # 1. LLM -> Code & extract code & write code
        code_solution_content, response_time = ask_llm(client=client, llm_model=args.llm_model, messages=messages)
        code_solution_content = extract_python_code(content=code_solution_content)

        sol_file_path = f'{base_solution_path}/{task_name}/{city_num}/{file_base_name}/{instance_tid}/{outer_tid}/solution_r{i}.py'
        write_py_file(path=sol_file_path, content=code_solution_content)

        # 2. Execute the code
        log_file_path = f'{base_log_path}/{task_name}/{city_num}/{file_base_name}/{instance_tid}/{outer_tid}/log_r{i}.txt'
        exec_status_str, execution_time, log_file_path = run_py_file(
            code_path=sol_file_path, log_path=log_file_path, max_exec_time=MAXIMUM_EXEC_TIME
        )

        # 3. Check execution results
        file_empty_str = 'EMPTY'
        if exec_status_str == 'success':
            # Check the content in log_file_path, if it is empty, then return 'fail', i, log_file_path
            file_empty_str = check_log_file_empty(file_path=log_file_path)
            if file_empty_str != 'EMPTY':
                return 'success', i, log_file_path

        # Setup some variables
        tmp_reflect_num = i
        tmp_log_file_path = log_file_path

        # 3.1 Assistant: code generated by LLM
        response_solution = {"role": "assistant", "content": code_solution_content}
        messages.append(response_solution)

        # 3.2 User: prompt to fix error
        # 3.2.1 Extract exec results
        log_content = read_txt_file(path=log_file_path)
        clipped_log_content = limit_text(text=log_content, max_length=MAXIMUM_TEXT_LENGTH)

        # 3.2.2 Construct prompt
        if file_empty_str == 'EMPTY':
            fix_error_prompt = (
                f'Although the generated solution executed successfully, '
                f'the execution results does not output anything, such as tour information or travel cost.\n'
                f'Please check the code and try again.'
            )
            fix_error_message = f"User - execution does not output any results.\n"
        elif exec_status_str == 'timeout':
            fix_error_prompt = (
                f'Here is the execution information:\n'
                f'{clipped_log_content}\n'
                f'The generated code exceeds the time limit of {MAXIMUM_EXEC_TIME} seconds.\n\n'
                f'Please generate a more time-efficient method, using heuristics or approximation techniques if necessary.'
            )
            fix_error_message = f"User - execution timeout. Regenerate solutions.\n"
        elif exec_status_str == 'error':
            fix_error_prompt = (
                f'The generated code has bugs. Here is the execution information:\n'
                f'{clipped_log_content}\n'
                f'Please fix all bugs. If fixing the bugs is too complex, you may generate a new solution. '
                f'Feel free to use heuristics or approximation techniques if necessary.'
            )
            fix_error_message = f"User - ask LLM to regenerate code by fixing bugs.\n"
        else:
            raise ValueError(f"Unknown execution status: {exec_status_str}")

        # 3.3 User: append to messages
        user_prompt = {"role": "user", "content": fix_error_prompt}
        messages.append(user_prompt)

        with open(messages_path, 'a') as file:
            file.write(fix_error_message)

        # 4. Maintain exec_detail_path  & message path
        with open(exec_detail_path, 'a') as file:
            file.write(f"Ask LLM for code solution, response time: {response_time:.2f} seconds.\n")
            file.write(f"Code solution execution status: {exec_status_str}, execution time: {execution_time}\n")
        with open(messages_path, 'a') as file:
            file.write(f"Assistant - Solution: {sol_file_path}\n")

    return 'fail', tmp_reflect_num, tmp_log_file_path


def get_executable_unit_test_code(args, client, extract_constraints_messages, task_name, city_num, file_base_name,
                                  llm_exec_reflect_num, base_verifier_log_path, base_verifier_path, exec_detail_path,
                                  log_file_path, constraints_content, messages_path, instance_tid, outer_tid):
    overall_verifier_prompt = ''
    unit_test_status = ''
    unit_test_res = ''
    for ui in range(args.reflect_num):
        # 1. Construct user prompt
        if ui == 0:
            log_content = read_txt_file(path=log_file_path)
            clipped_log_content = limit_text(text=log_content, max_length=MAXIMUM_TEXT_LENGTH)
            verifier_prompt = (
                'Here is the solution:\n'
                f'{clipped_log_content}\n'
                # f'If {clipped_log_content} does not contain solutions, output "FAIL".\n'
                'Please generate unit tests by using Python code to verify if the solution is correct by checking the following requirements:\n'
                f'{constraints_content}\n'
                'If the solution is correct, output "CORRECT"; otherwise, output "FAIL".'
            )
        else:
            verifier_prompt = overall_verifier_prompt

        user_prompt = {"role": "user", "content": verifier_prompt}
        extract_constraints_messages.append(user_prompt)

        # 2. Verifier LLM: generate unit tests code
        verifier_code_content, response_time = ask_llm(client=client, llm_model=args.llm_model,
                                                       messages=extract_constraints_messages)
        verifier_code_content = extract_python_code(content=verifier_code_content)

        # 3. Write verifier code to file & execute verifier code
        verifier_file_path = f'{base_verifier_path}/{task_name}/{city_num}/{file_base_name}/{instance_tid}/{outer_tid}/verifier_r{llm_exec_reflect_num}_v{ui}.py'
        write_py_file(path=verifier_file_path, content=verifier_code_content)
        verifier_log_file_path = f'{base_verifier_log_path}/{task_name}/{city_num}/{file_base_name}/{instance_tid}/{outer_tid}/verifier_log_r{llm_exec_reflect_num}_v{ui}.txt'
        verifier_exec_status_str, execution_time, verifier_log_file_path = run_py_file(
            code_path=verifier_file_path, log_path=verifier_log_file_path,
            max_exec_time=MAXIMUM_EXEC_TIME
        )

        # 4. Check verifier execution status
        if verifier_exec_status_str == 'success':
            verify_res_str = check_correct_in_file(file_path=verifier_log_file_path)
            if verify_res_str == 'CORRECT':
                with open(exec_detail_path, 'a') as file:
                    file.write(f"Ask another LLM for verifier code, response time: {response_time:.2f} seconds.\n")
                    file.write(f"Another LLM - pass verifier\n")
                return 'success', 'CORRECT'
            elif verify_res_str == 'FAIL':
                with open(exec_detail_path, 'a') as file:
                    file.write(f"Ask another LLM for verifier code, response time: {response_time:.2f} seconds.\n")
                    file.write(f"Another LLM - fail verifier\n")
                return 'success', 'FAIL'
            elif verify_res_str == 'None':
                unit_test_status = 'success'
                unit_test_res = 'None'

                verify_log_content = read_txt_file(path=verifier_log_file_path)
                clipped_verify_log_content = limit_text(text=verify_log_content, max_length=MAXIMUM_TEXT_LENGTH)
                overall_verifier_prompt = (
                    f'The generated unit tests for verification does not output "CORRECT" or "FAIL". Here is the executed information:\n'
                    f'{clipped_verify_log_content}\n'
                    'Please regenerate unit tests by using Python code.\n'
                    'If the solution is correct, output "CORRECT"; otherwise, output "FAIL".'
                )

                with open(exec_detail_path, 'a') as file:
                    file.write(f"Ask another LLM for verifier code, response time: {response_time:.2f} seconds.\n")
                    file.write(f"Another LLM - verifier does not output CORRECT OR FAIL.\n")
        else:
            unit_test_status = 'fail'
            unit_test_res = 'not_executed'

            # Verifier: user prompt
            verify_log_content = read_txt_file(path=verifier_log_file_path)
            clipped_verify_log_content = limit_text(text=verify_log_content, max_length=MAXIMUM_TEXT_LENGTH)
            overall_verifier_prompt = (
                f'The generated unit tests for verification have bugs. Here is the executed information:\n'
                f'{clipped_verify_log_content}\n'
                f'Please fix all bugs. If fixing bugs is too complex, you may generate new unit tests by using Python code.'
            )

            with open(exec_detail_path, 'a') as file:
                file.write(f"Ask another LLM for verifier code, response time: {response_time:.2f} seconds.\n")
                file.write(f"Another LLM - The verifier has bugs.\n")

        # verify_res_str == 'None' or verifier_exec_status_str == 'error' / 'timeout'
        response_verifier = {"role": "assistant", "content": verifier_code_content}
        extract_constraints_messages.append(response_verifier)

        with open(messages_path, 'a') as file:
            file.write(f"Another LLM - User - Verifier prompt.\n")
            file.write(f"Another LLM - Assistant - Verifier: {verifier_file_path}\n")

    return unit_test_status, unit_test_res


def get_results_one_try(
        args, client, file_path, file_base_name, task_name, city_num, track_file_path,
        base_exec_details_path, base_messages_path, base_solution_path, base_log_path, base_constraints_path,
        base_verifier_path, base_verifier_log_path, instance_tid, outer_tid):
    # 1. Write start info;
    # a) Track info: Reflect times and results path
    # b) Execution info: The process of each instance execution
    # c) Message info: How the message construct
    exec_detail_path, start_time, messages_path = write_start_info(
        track_file_path=track_file_path, file_base_name=file_base_name,
        base_exec_details_path=base_exec_details_path, task_name=task_name, city_num=city_num,
        base_messages_path=base_messages_path, instance_tid=instance_tid, outer_tid=outer_tid
    )

    # 2. Load the task description
    task_description = read_txt_file(path=file_path)

    # 3. Construct the messages
    messages = [
        {"role": "system", "content": LLM_SYSTEM_PROMPT},
        {"role": "user", "content": task_description}
    ]
    with open(messages_path, 'a') as file:
        file.write(f"System - LLM_SYSTEM_PROMPT\n")
        file.write(f"User - Task description: {file_base_name}\n")

    # 4. Task -> Solution & Execution -> Results -> exec_status_str
    exec_status_str, llm_exec_reflect_num, tmp_log_file_path = task2exec_res(
        args=args, client=client, file_base_name=file_base_name, task_name=task_name, city_num=city_num,
        messages=messages, base_solution_path=base_solution_path, base_log_path=base_log_path,
        exec_detail_path=exec_detail_path, messages_path=messages_path, instance_tid=instance_tid, outer_tid=outer_tid
    )

    # 5. Check exec_status_str, exec_status_str = fail
    # Check the content in tmp_log_file_path, if it is empty, then return 'fail', 'None', 'None'
    file_empty_str = check_log_file_empty(file_path=tmp_log_file_path)
    if exec_status_str == 'fail' or file_empty_str == 'EMPTY':
        write_end_info(start_time=start_time, exec_detail_path=exec_detail_path,
                       track_file_path=track_file_path, tmp_reflect_num=llm_exec_reflect_num,
                       tmp_log_file_path=tmp_log_file_path)
        # return exec_status_str, unit_test_status, unit_test_res
        return 'fail', 'None', 'None'

    # 7 exec_status_str = success -> verifier
    # 7.1 Get constraints from task description
    extract_constraints_prompt = (
        'Following is the task description for a variant of the TSP (Traveling Salesman Problem) or VRP (Vehicle Routing Problem):\n'
        '---\n'
        f'{task_description}\n'
        '---\n'
        'Please extract all constraints from the given task description and list them in the following format:\n'
        '- [Requirement 1]\n'
        '- [Requirement 2]\n'
        '- [Requirement 3]\n'
        'Ensure the output contains only the list of constraints in the specified format and no additional information.'
    )

    extract_constraints_messages = [
        {"role": "system", "content": LLM_SYSTEM_PROMPT},
        {"role": "user", "content": extract_constraints_prompt}
    ]

    # 7.2 Ask LLM for constraints -> Write down constraints
    constraints_content, response_time = ask_llm(
        client=client, llm_model=args.llm_model, messages=extract_constraints_messages
    )
    constraints_path = f'{base_constraints_path}/{task_name}/{city_num}/{file_base_name}/{instance_tid}/{outer_tid}/constraints_r{llm_exec_reflect_num}.txt'
    os.makedirs(os.path.dirname(constraints_path), exist_ok=True)
    with open(constraints_path, 'w') as file:
        file.write(constraints_content)

    # 7.3 Construct constraints response
    constraints_content_response = {"role": "assistant", "content": constraints_content}
    extract_constraints_messages.append(constraints_content_response)

    # 7.4 Maintain messages_path, exec_detail_path
    with open(exec_detail_path, 'a') as file:
        file.write(f"Another LLM - Get constraints from LLM, response time: {response_time:.2f} seconds.\n")
    with open(messages_path, 'a') as file:
        file.write(f"Another LLM - User - Ask for prompts\n")
        file.write(f"Another LLM - Assistant - Constraints response: {constraints_path}\n")

    unit_test_status, unit_test_res = get_executable_unit_test_code(
        args=args, client=client, extract_constraints_messages=extract_constraints_messages,
        task_name=task_name, city_num=city_num, file_base_name=file_base_name,
        llm_exec_reflect_num=llm_exec_reflect_num, base_verifier_log_path=base_verifier_log_path,
        base_verifier_path=base_verifier_path, exec_detail_path=exec_detail_path,
        log_file_path=tmp_log_file_path, constraints_content=constraints_content,
        messages_path=messages_path, instance_tid=instance_tid, outer_tid=outer_tid
    )

    if unit_test_status == 'success':
        if unit_test_res == 'CORRECT':
            with open(track_file_path, 'a') as file:
                file.write(f"Get results, pass the verifier.\n\n")

            # return exec_status_str, unit_test_status, unit_test_res
            return 'success', 'success', 'CORRECT'
        elif unit_test_res == 'FAIL':
            with open(track_file_path, 'a') as file:
                file.write(f"Get results, can not pass the verifier.\n\n")
            # return exec_status_str, unit_test_status, unit_test_res
            return 'success', 'success', 'FAIL'
        elif unit_test_res == 'None':
            with open(track_file_path, 'a') as file:
                file.write(f"Get results, but the verifier does not output CORRECT or FAIL.\n\n")

            # return exec_status_str, unit_test_status, unit_test_res
            return 'success', 'success', 'None'
    elif unit_test_status == 'fail':
        with open(track_file_path, 'a') as file:
            file.write(f"Get results, but the verifier has bugs.\n\n")

        # return exec_status_str, unit_test_status, unit_test_res
        return 'success', 'fail', 'not_executed'
    else:
        raise ValueError(f"Unknown unit test status: {unit_test_status}")


def solve_batch(args):
    # TODO: Get execution time of optimal solutions provided by guangyao, the maximum exec time is less than 2x of that time.
    # Setup client
    client = OpenAI(api_key=OPENAI_API_KEY)

    path_prex = 'extra_info'
    base_task_path = f'{BASE_PATH}/task/{args.robot_num}'

    # LLM executor
    base_solution_path = f'{BASE_PATH}/{path_prex}/solution/{args.robot_num}'
    base_log_path = f'{BASE_PATH}/{path_prex}/log/{args.robot_num}'
    base_exec_details_path = f'{BASE_PATH}/{path_prex}/exec_details/{args.robot_num}'
    base_messages_path = f'{BASE_PATH}/{path_prex}/messages/{args.robot_num}'

    # LLM verifier
    base_constraints_path = f'{BASE_PATH}/{path_prex}/constraints/{args.robot_num}'
    base_verifier_path = f'{BASE_PATH}/{path_prex}/verifier/{args.robot_num}'
    base_verifier_log_path = f'{BASE_PATH}/{path_prex}/log_verifier/{args.robot_num}'

    base_track_file_path = f'{BASE_PATH}/{path_prex}/track/{args.robot_num}'

    for city_num in CITY_NUM_LIST:

        if args.robot_num == 'single':
            cur_task_list = SINGLE_TASK_LIST
        elif args.robot_num == 'multiple':
            cur_task_list = MULTI_TASK_LIST
        else:
            raise ValueError(f"Unknown robot_num: {args.robot_num}")

        for task_name in cur_task_list:
            task_folder = os.path.join(base_task_path, task_name)
            file_path_list, file_name_list = list_files(directory=task_folder)
            file_path_list.sort()
            file_name_list.sort()

            # Maintain exec_detail_path
            for file_path, file_name in zip(file_path_list, file_name_list):
                # 0. Extract city number from file name
                file_city_num = extract_value(file_name=file_name)
                if int(file_city_num) != city_num:
                    continue

                print(f"Task file path: {file_path}")
                file_base_name = file_name[:-4]

                # 1. Instance try 5 times
                for instance_tid in range(INSTANCE_TRY_TIMES):
                    final_success_bool = False
                    track_file_path = f'{base_track_file_path}/{task_name}/{city_num}/{instance_tid}/track.txt'
                    os.makedirs(os.path.dirname(track_file_path), exist_ok=True)
                    # Create track file
                    with open(track_file_path, 'w') as file:
                        file.write(f"Task: {task_name}, City: {city_num}, Run time: {instance_tid}\n")

                    for outer_tid in range(args.reflect_num):
                        exec_status_str, unit_test_status, unit_test_res = get_results_one_try(
                            args=args, client=client, file_path=file_path, file_base_name=file_base_name,
                            task_name=task_name, city_num=city_num, track_file_path=track_file_path,
                            base_exec_details_path=base_exec_details_path, base_messages_path=base_messages_path,
                            base_solution_path=base_solution_path, base_log_path=base_log_path,
                            base_constraints_path=base_constraints_path, base_verifier_path=base_verifier_path,
                            base_verifier_log_path=base_verifier_log_path, instance_tid=instance_tid,
                            outer_tid=outer_tid
                        )
                        if exec_status_str == 'success' and unit_test_status == 'success' and unit_test_res == 'CORRECT':
                            final_success_bool = True
                            break

                    if final_success_bool:
                        with open(track_file_path, 'a') as file:
                            file.write(f"Finally get correct results.\n")
                    else:
                        with open(track_file_path, 'a') as file:
                            file.write(f"Can not get results.\n")


def solve_problem(args):
    solve_batch(args=args)


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--llm_model', type=str, default='gpt-4-turbo',
                        choices=['gpt-4-turbo-2024-04-09', 'gpt-4o-2024-05-13', 'gpt-4o-mini-2024-07-18'])
    parser.add_argument('--reflect_num', type=int, default=4, help='Default: self reflect 3 times.')
    parser.add_argument('--robot_num', type=str, default='single', choices=['single', 'multiple'],
                        help='Default: self reflect 5 times.')
    return parser.parse_args()


def main():
    args = get_args()
    solve_problem(args=args)


if __name__ == '__main__':
    sys.exit(main())
